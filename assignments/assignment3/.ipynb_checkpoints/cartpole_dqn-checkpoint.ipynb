{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 - Reinforcement Learning with Deep Q-Networks\n",
    "\n",
    "#### Kobee Raveendran\n",
    "#### CAP 5610"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports (Gym environment and Deep Q-Network dependencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent and Environment (Explanation)\n",
    "\n",
    "#### Agent\n",
    "The agent in the CartPole environment is simply the cart itself; it is responsible for doing the \"learning\" in this problem. The agent interacts with the environment and learns optimal actions that correspond to higher rewards through experience over time. It gets a sense for which actions are optimal by first performing the action, then receiving a reward from the observation of the next state given by the environment (which repeats in a cyclic fashion).\n",
    "\n",
    "#### Agent Action Space\n",
    "In this environment, there is a defined action space that represents the set of actions an agent can perform at each time step. In the CartPole environment, the cart (agent) can only perform one of two actions: move left or right. These two actions constitute the action space, and can be confirmed by viewing the elements or size of the action space (as in `env.action_space.n`).\n",
    "\n",
    "#### Observation Space\n",
    "Since this is a relatively simple environment, the observation space, which is the simplest representation for the current state of the environment at each time step, is fairly small. It consists of only 4 elements (confirmed by viewing the shape of `env.observation_space`), which are the **cart position**, **cart velocity**, **pole angle**, and **velocity of the pole at the tip** (confirmed by the cartpole environment source code).\n",
    "\n",
    "#### Rewards\n",
    "In the CartPole environment, rewards are fairly straightforward. For every time step that the pole is aloft and in a \"surviving\" (non-terminal) state, the agent receives a reward of +1. A state is deemed terminal if the pole's angle surpasses a set threshold (+/- 12 degrees) or if the cart has passed a certain distance away from the center (2.4 units), thus reaching the edge of the display (according to the CartPole source code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cartpole(param_name:str, param_vals):\n",
    "    \n",
    "    # set the default parameters (of which only one will vary per function call)\n",
    "    params = {\n",
    "        'learning_rate': 0.001, \n",
    "        'gamma': 0.95, \n",
    "        'epsilon': 1.0, \n",
    "        'epsilon_min': 0.01, \n",
    "        'epsilon_decay': 0.995\n",
    "    }\n",
    "    \n",
    "    print('\\n\\nPARAM: {}'.format(param_name))\n",
    "    \n",
    "    \n",
    "    \n",
    "    for param_val in param_vals:\n",
    "        env = gym.make('CartPole-v0')\n",
    "        \n",
    "        obs_space = env.observation_space.shape[0]\n",
    "        action_space = env.action_space.n    \n",
    "        \n",
    "        params[param_name] = param_val\n",
    "        \n",
    "        print('\\nVAL: {}'.format(param_val))\n",
    "        \n",
    "        dqn = DQN(\n",
    "            obs_space, \n",
    "            action_space, \n",
    "            params['learning_rate'], \n",
    "            params['gamma'], \n",
    "            params['epsilon'], \n",
    "            params['epsilon_min'], \n",
    "            params['epsilon_decay']\n",
    "        )\n",
    "        \n",
    "        scores = []\n",
    "        \n",
    "        for episode in range(100):\n",
    "            state = env.reset()\n",
    "            state.reshape([1, obs_space])\n",
    "\n",
    "            time_step = 0\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                env.render()\n",
    "\n",
    "                action = dqn.act(state)\n",
    "\n",
    "                observation, reward, done, info = env.step(action)\n",
    "\n",
    "                if done:\n",
    "                    observation = env.reset()\n",
    "                    #print('Terminal observation: ', observation)\n",
    "\n",
    "                time_step += 1\n",
    "\n",
    "                dqn.experience_replay()\n",
    "\n",
    "            scores.append(time_step)\n",
    "\n",
    "        mean_survival_time = np.mean(scores)\n",
    "        \n",
    "\n",
    "        print('Mean survival time: ', mean_survival_time)\n",
    "    \n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up neural network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "PARAM: learning_rate\n",
      "\n",
      "VAL: 0.001\n",
      "Mean survival time:  21.47\n",
      "\n",
      "VAL: 0.01\n",
      "Mean survival time:  22.44\n",
      "\n",
      "VAL: 0.1\n",
      "Mean survival time:  22.44\n",
      "\n",
      "VAL: 1.0\n",
      "Mean survival time:  21.75\n",
      "\n",
      "VAL: 10.0\n",
      "Mean survival time:  20.77\n"
     ]
    }
   ],
   "source": [
    "class DQN:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 obs_space, \n",
    "                 action_space, \n",
    "                 learning_rate = 0.001, \n",
    "                 gamma = 0.95, \n",
    "                 epsilon = 1.0, \n",
    "                 epsilon_min = 0.01,\n",
    "                 epsilon_decay = 0.995\n",
    "                ):\n",
    "        self.expl_rate = 1.0\n",
    "        self.action_space = action_space\n",
    "        \n",
    "        self.memory = deque(maxlen = 2000)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        \n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(24, input_dim = obs_space, activation = 'relu'))\n",
    "        self.model.add(Dense(24, activation = 'relu'))\n",
    "        self.model.add(Dense(self.action_space, activation = 'linear'))\n",
    "        \n",
    "        self.model.compile(loss = 'mse', optimizer = Adam(lr = self.learning_rate))\n",
    "        \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    def act(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return random.randrange(self.action_space)\n",
    "        \n",
    "        q_vals = self.model.predict(state)\n",
    "        \n",
    "        print(q_vals)\n",
    "        \n",
    "    def experience_replay(self):\n",
    "        if len(self.memory) < 20:\n",
    "            return\n",
    "        \n",
    "        batch = random.sample(self.memory, 20)\n",
    "        \n",
    "        for state, action, reward, state_next, done in batch:\n",
    "            q_update = reward\n",
    "            \n",
    "            if not done:\n",
    "                q_update = (reward + gamma * np.amax(self.model.predict(state_next)[0]))\n",
    "                \n",
    "            q_values = self.model.predict(state)\n",
    "            \n",
    "            q_values[0][action] = q_update\n",
    "            \n",
    "            self.model.fit(state, q_values, verbose = 0)\n",
    "            \n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            self.epsilon = np.argmax(self.epsilon, self.epsilon_min)\n",
    "            \n",
    "\n",
    "# test param configurations in standalone runs (some configs will be duplicated as they match the default vals)\n",
    "cartpole('learning_rate', [0.001, 0.01, 0.1, 1.0, 10.0])\n",
    "#cartpole('gamma', [0.25, 0.5, 0.75, 0.95])\n",
    "#cartpole('epsilon', [1.0, 1.5, 10.0])\n",
    "#cartpole('epsilon_min', [0.001, 0.01, 0.1, 1.0])\n",
    "#cartpole('epsilon_decay', [0.75, 0.9, 0.995, 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
